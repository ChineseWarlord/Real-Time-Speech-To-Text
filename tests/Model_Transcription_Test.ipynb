{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic libraries and initiate functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import threading\n",
    "import keyboard\n",
    "import pyaudio\n",
    "import whisper\n",
    "import psutil\n",
    "import queue\n",
    "import wave\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def get_models(models):\n",
    "    print(\"Available models:\")\n",
    "    list = []\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"{i}: {model}\")\n",
    "        list.append(model)\n",
    "    return list\n",
    "        \n",
    "def run_model(type:str,models:str,workers:int):\n",
    "    if type == \"openai\":\n",
    "        try:\n",
    "            model = whisper.load_model(models)\n",
    "            return model\n",
    "        except:\n",
    "            print(\"Error occured!\")\n",
    "    elif type == \"ctranslate\":\n",
    "        try:\n",
    "            model = WhisperModel(models,device=\"cuda\",compute_type=\"int8_float16\",num_workers=workers)\n",
    "            return model\n",
    "        except:\n",
    "            print(\"Error occured!\")\n",
    "            \n",
    "def transcribe_fast(model,file: Union[str,np.ndarray],size:int,y:bool,print_out:bool):\n",
    "    model = model\n",
    "    \n",
    "    # convert audio data buffer to a NumPy ndarray\n",
    "    #audio_array = np.frombuffer(file, dtype=np.int16)\n",
    "    \n",
    "    # Return segments and info such as detected lang. and prob.\n",
    "    segments, info = model.transcribe(audio=file,beam_size=size,word_timestamps=y)\n",
    "    text = segments\n",
    "    if print_out == True:\n",
    "        print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability), end=\"\")\n",
    "\n",
    "    # Print out segments\n",
    "    if print_out == True:\n",
    "        for segment in segments:\n",
    "            print(\"\\n[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text), end=\"\")\n",
    "            \n",
    "    return segments\n",
    "  \n",
    "def transcribe_openai(model, file):\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(file)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions()\n",
    "    result = whisper.decode(model, mel, options)\n",
    "\n",
    "    # print the recognized text\n",
    "    print(result.text)\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(\"audio.mp3\")\n",
    "  \n",
    "def translate_fast(model,file: Union[str,np.ndarray],size:int,y:bool,print_out:bool):\n",
    "    #model = model\n",
    "    # Return segments and info such as detected lang. and prob.\n",
    "    segments, info = model.transcribe(audio=file,beam_size=size,word_timestamps=y,task=\"translate\")\n",
    "    list = []\n",
    "    \n",
    "    if print_out == True:\n",
    "        print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "    # Print out segments\n",
    "    if print_out == True:\n",
    "        for segment in segments:\n",
    "            print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "            list.append(segment.text)\n",
    "            print(f\"items in list: {segment.text}\")\n",
    "            \n",
    "    return list\n",
    "\n",
    "def translate_openai(model,file):\n",
    "    model = model\n",
    "    result = model.transcribe(file,task=\"translate\")\n",
    "    print(result[\"text\"])\n",
    "    \n",
    "def record_audio(audio_obj,mic_id,audio_buffer):\n",
    "    # Recording parameters\n",
    "    chunk = 1024\n",
    "    format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "    rate = 44100\n",
    "    rec_sec = 0.5\n",
    "    \n",
    "    stream = audio_obj.open(format=format, \n",
    "                        channels=channels, \n",
    "                        rate=rate, \n",
    "                        input=True,\n",
    "                        input_device_index=mic_id,\n",
    "                        frames_per_buffer=chunk)\n",
    "    print(\"Recording!\")\n",
    "    frames = []\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(audio_obj.get_sample_size(format))\n",
    "    wf.setframerate(rate)\n",
    "    while 1:\n",
    "        for i in range(0,int(rate / chunk * rec_sec)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "            audio_buffer.put(audio_data)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "            \n",
    "        if keyboard.is_pressed(\"q\"):\n",
    "            # stop recording\n",
    "            wf.close()\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            audio_obj.terminate()\n",
    "            break\n",
    "    \n",
    "    print(\"Closing thread...\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available audio sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Device id  0  -  Microsoft Sound Mapper - Input\n",
      "Input Device id  1  -  Chat Mic (TC-HELICON GoXLR)\n",
      "Input Device id  2  -  Broadcast Stream Mix (TC-HELICO\n",
      "Input Device id  3  -  Sample (TC-HELICON GoXLR)\n",
      "Input Device id  4  -  IndgÃ¥ende linje (Realtek(R) Aud\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "# Get a list of available input devices\n",
    "audio_obj= pyaudio.PyAudio()\n",
    "info = audio_obj.get_host_api_info_by_index(0)\n",
    "numdevices = info.get('deviceCount')\n",
    "device_id = []\n",
    "for i in range(0, numdevices):\n",
    "    if (audio_obj.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "        print(\"Input Device id \", i, \" - \", audio_obj.get_device_info_by_host_api_device_index(0, i).get('name'))\n",
    "        device_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize audio buffer\n",
    "audio_buffer = queue.Queue()\n",
    "\n",
    "# Create audio recording thread\n",
    "#t1 = threading.Thread(target=record_audio, args=(audio_obj,device_id[5],audio_buffer))\n",
    "#t1.start()\n",
    "\n",
    "#p1 = Process(target=record_audio,args=(audio_obj,device_id[5],queue))\n",
    "#p1.start()\n",
    "#record_audio(audio_obj,device_id[5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for available models, then download the preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "0: tiny.en\n",
      "1: tiny\n",
      "2: base.en\n",
      "3: base\n",
      "4: small.en\n",
      "5: small\n",
      "6: medium.en\n",
      "7: medium\n",
      "8: large-v1\n",
      "9: large-v2\n",
      "10: large\n"
     ]
    }
   ],
   "source": [
    "models = whisper.available_models()\n",
    "id = get_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_model(\"ctranslate\",id[9],4)\n",
    "#model2 = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run and time execution of transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected language 'ja' with probability 0.956055\n",
      "[0.00s -> 0.82s] また明日!\n",
      "items in list: ['また明日!']\n",
      "\n",
      "\n",
      "Elapsed time: 0.5575037002563477\n",
      "Average time: 0.5575037002563477 s\n",
      "Total time: 0.5575037002563477 s\n",
      "\n",
      "Detected language 'ja' with probability 0.956055\n",
      "[0.00s -> 0.76s]  See you tomorrow.\n",
      "items in list: [' See you tomorrow.']\n",
      "\n",
      "\n",
      "Elapsed time: 0.4767131805419922\n",
      "Average time: 1.0342168807983398 s\n",
      "Total time: 1.0342168807983398 s\n"
     ]
    }
   ],
   "source": [
    "file = \"output.wav\"\n",
    "file = \"Audio/jap4.wav\"\n",
    "t_avg = []\n",
    "list = []\n",
    "n = 1\n",
    "\n",
    "# Transcribe audio\n",
    "for i in range(n):\n",
    "    t1 = time.time()\n",
    "    #text = transcribe_fast(model,file,5,True,True)\n",
    "    segments, info = model.transcribe(audio=file,beam_size=5,word_timestamps=True,task=\"transcribe\")\n",
    "    print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "    # Print out segments\n",
    "    for segment in segments:\n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "        list.append(segment.text)\n",
    "        print(f\"items in list: {list}\")\n",
    "    t2 = time.time()\n",
    "    print(\"\\n\\nElapsed time:\", t2-t1)\n",
    "    t_avg.append(t2-t1)\n",
    "print(f\"Average time: {np.sum(t_avg)/n} s\")\n",
    "print(f\"Total time: {np.sum(t_avg)} s\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "list2 = []\n",
    "# Translate audio\n",
    "for i in range(n):\n",
    "    t1 = time.time()\n",
    "    #text = transcribe_fast(model,file,5,True,True)\n",
    "    segments, info = model.transcribe(audio=file,beam_size=5,word_timestamps=True,task=\"translate\")\n",
    "    print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "    # Print out segments\n",
    "    for segment in segments:\n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "        list2.append(segment.text)\n",
    "        print(f\"items in list: {list2}\")\n",
    "    t2 = time.time()\n",
    "    print(\"\\n\\nElapsed time:\", t2-t1)\n",
    "    t_avg.append(t2-t1)\n",
    "print(f\"Average time: {np.sum(t_avg)/n} s\")\n",
    "print(f\"Total time: {np.sum(t_avg)} s\")\n",
    "\n",
    "\n",
    "#while 1:\n",
    "#    #audio_data = audio_buffer.get()\n",
    "#    t1 = time.time()\n",
    "#    text = transcribe_fast(model,file,5,True,True)\n",
    "#    t2 = time.time()\n",
    "#    print(\"\\nElapsed time:\", t2-t1, end=\"\")\n",
    "    \n",
    "#    if keyboard.is_pressed(\"w\"):\n",
    "#            print(\"Closing thread...\")\n",
    "#            print(\"\\nStopping...\")\n",
    "#            sys.exit()\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "また明日!\n"
     ]
    }
   ],
   "source": [
    "for i in list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[またあした] /(exp) see you tomorrow/\n",
      "time 0.33006882667541504\n"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(to_lang=\"en\",from_lang=\"ja\")\n",
    "for i in list:\n",
    "    t1 = time.time()\n",
    "    translation = translator.translate(i)\n",
    "    print(translation)\n",
    "    print(\"time\",time.time()-t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['see you tomorrow!']\n",
      "time 0.13094305992126465\n",
      "['see you tomorrow!']\n",
      "time 0.13263821601867676\n",
      "['see you tomorrow!']\n"
     ]
    }
   ],
   "source": [
    "from deep_translator import GoogleTranslator, MyMemoryTranslator, LibreTranslator,batch_detection\n",
    "t1 = time.time()\n",
    "trans = GoogleTranslator(\"ja\",\"en\").translate_batch(list)\n",
    "print(trans)\n",
    "print(\"time\",time.time()-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "trans = GoogleTranslator(\"ja\",\"en\").translate(str(list))\n",
    "print(trans)\n",
    "print(\"time\",time.time()-t1)\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ma', '/(aux-v) (1) indicate past completed or action/(2) indicates light imperative/', 'Meiji ', 'Day', '!']\n",
      "time 1.3724980354309082\n",
      "[またあした] /(exp) see you tomorrow/\n",
      "time 0.2812323570251465\n"
     ]
    }
   ],
   "source": [
    "for i in list:\n",
    "    t1 = time.time()\n",
    "    translated = MyMemoryTranslator(source='ja', target='en').translate_batch(str(i))\n",
    "    print(translated)\n",
    "    print(\"time\",time.time()-t1)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    translated = MyMemoryTranslator(source='ja', target='en').translate(str(list))\n",
    "    print(translated)\n",
    "    print(\"time\",time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow and bad accuracy\n",
    "#translated = LibreTranslator(source='ja', target='en').translate_batch(list)\n",
    "#print(translated)\n",
    "#translated = LibreTranslator(source='ja', target='en').translate(str(list))\n",
    "#print(translated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage: 1865.484375 MB\n"
     ]
    }
   ],
   "source": [
    "pid = psutil.Process()\n",
    "memory_info = pid.memory_info()\n",
    "print(\"Memory usage:\", memory_info.rss / 1024 / 1024, \"MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
