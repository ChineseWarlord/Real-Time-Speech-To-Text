{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import basic libraries and initiate functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "g:\\Github\\PRIVATE PROJECTS\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from faster_whisper import WhisperModel\n",
    "from typing import Union\n",
    "import numpy as np\n",
    "import threading\n",
    "import keyboard\n",
    "import pyaudio\n",
    "import whisper\n",
    "import psutil\n",
    "import queue\n",
    "import wave\n",
    "import time\n",
    "import sys\n",
    "\n",
    "def get_models(models):\n",
    "    print(\"Available models:\")\n",
    "    list = []\n",
    "    for i, model in enumerate(models):\n",
    "        print(f\"{i}: {model}\")\n",
    "        list.append(model)\n",
    "    return list\n",
    "        \n",
    "def run_model(type:str,models:str,workers:int, device:str):\n",
    "    if type == \"openai\":\n",
    "        try:\n",
    "            model = whisper.load_model(models)\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured! {e}\")\n",
    "    elif type == \"ctranslate\":\n",
    "        try:\n",
    "            #model = WhisperModel(models,device=device,compute_type=\"int8_float16\",num_workers=workers, download_root=\"../models/\") # GPU ONLY\n",
    "            model = WhisperModel(models,device=device,num_workers=workers, download_root=\"../models/\") # CPU ONLY\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error occured! {e}\")\n",
    "            \n",
    "def transcribe_fast(model,file: Union[str,np.ndarray],size:int,y:bool,print_out:bool):\n",
    "    model = model\n",
    "    \n",
    "    # convert audio data buffer to a NumPy ndarray\n",
    "    #audio_array = np.frombuffer(file, dtype=np.int16)\n",
    "    \n",
    "    # Return segments and info such as detected lang. and prob.\n",
    "    segments, info = model.transcribe(audio=file,beam_size=size,word_timestamps=y)\n",
    "    text = segments\n",
    "    if print_out == True:\n",
    "        print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability), end=\"\")\n",
    "\n",
    "    # Print out segments\n",
    "    if print_out == True:\n",
    "        for segment in segments:\n",
    "            print(\"\\n[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text), end=\"\")\n",
    "            \n",
    "    return segments\n",
    "  \n",
    "def transcribe_openai(model, file):\n",
    "    # load audio and pad/trim it to fit 30 seconds\n",
    "    audio = whisper.load_audio(file)\n",
    "    audio = whisper.pad_or_trim(audio)\n",
    "\n",
    "    # make log-Mel spectrogram and move to the same device as the model\n",
    "    mel = whisper.log_mel_spectrogram(audio).to(model.device)\n",
    "\n",
    "    # detect the spoken language\n",
    "    _, probs = model.detect_language(mel)\n",
    "    print(f\"Detected language: {max(probs, key=probs.get)}\")\n",
    "\n",
    "    # decode the audio\n",
    "    options = whisper.DecodingOptions()\n",
    "    result = whisper.decode(model, mel, options)\n",
    "\n",
    "    # print the recognized text\n",
    "    print(result.text)\n",
    "    model = whisper.load_model(\"base\")\n",
    "    result = model.transcribe(\"audio.mp3\")\n",
    "  \n",
    "def translate_fast(model,file: Union[str,np.ndarray],size:int,y:bool,print_out:bool):\n",
    "    #model = model\n",
    "    # Return segments and info such as detected lang. and prob.\n",
    "    segments, info = model.transcribe(audio=file,beam_size=size,word_timestamps=y,task=\"translate\")\n",
    "    list = []\n",
    "    \n",
    "    if print_out == True:\n",
    "        print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "    # Print out segments\n",
    "    if print_out == True:\n",
    "        for segment in segments:\n",
    "            print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "            list.append(segment.text)\n",
    "            print(f\"items in list: {segment.text}\")\n",
    "            \n",
    "    return list\n",
    "\n",
    "def translate_openai(model,file):\n",
    "    model = model\n",
    "    result = model.transcribe(file,task=\"translate\")\n",
    "    print(result[\"text\"])\n",
    "    \n",
    "def record_audio(audio_obj,mic_id,audio_buffer):\n",
    "    # Recording parameters\n",
    "    chunk = 1024\n",
    "    format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "    rate = 44100\n",
    "    rec_sec = 0.5\n",
    "    \n",
    "    stream = audio_obj.open(format=format, \n",
    "                        channels=channels, \n",
    "                        rate=rate, \n",
    "                        input=True,\n",
    "                        input_device_index=mic_id,\n",
    "                        frames_per_buffer=chunk)\n",
    "    print(\"Recording!\")\n",
    "    frames = []\n",
    "    WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "    wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(audio_obj.get_sample_size(format))\n",
    "    wf.setframerate(rate)\n",
    "    while 1:\n",
    "        for i in range(0,int(rate / chunk * rec_sec)):\n",
    "            data = stream.read(chunk)\n",
    "            frames.append(data)\n",
    "            audio_data = np.frombuffer(data, dtype=np.int16)\n",
    "            audio_buffer.put(audio_data)\n",
    "            wf.writeframes(b''.join(frames))\n",
    "            \n",
    "        if keyboard.is_pressed(\"q\"):\n",
    "            # stop recording\n",
    "            wf.close()\n",
    "            stream.stop_stream()\n",
    "            stream.close()\n",
    "            audio_obj.terminate()\n",
    "            break\n",
    "    \n",
    "    print(\"Closing thread...\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check available audio sources\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Device id  0  -  Microsoft Sound Mapper - Input\n",
      "Input Device id  1  -  Chat Mic (3- TC-HELICON GoXLR)\n",
      "Input Device id  2  -  Broadcast Stream Mix (3- TC-HEL\n",
      "Input Device id  3  -  Sample (3- TC-HELICON GoXLR)\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "\n",
    "# Get a list of available input devices\n",
    "audio_obj= pyaudio.PyAudio()\n",
    "info = audio_obj.get_host_api_info_by_index(0)\n",
    "numdevices = info.get('deviceCount')\n",
    "device_id = []\n",
    "for i in range(0, numdevices):\n",
    "    if (audio_obj.get_device_info_by_host_api_device_index(0, i).get('maxInputChannels')) > 0:\n",
    "        print(\"Input Device id \", i, \" - \", audio_obj.get_device_info_by_host_api_device_index(0, i).get('name'))\n",
    "        device_id.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize audio buffer\n",
    "audio_buffer = queue.Queue()\n",
    "\n",
    "# Create audio recording thread\n",
    "#t1 = threading.Thread(target=record_audio, args=(audio_obj,device_id[5],audio_buffer))\n",
    "#t1.start()\n",
    "\n",
    "#p1 = Process(target=record_audio,args=(audio_obj,device_id[5],queue))\n",
    "#p1.start()\n",
    "#record_audio(audio_obj,device_id[5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check for available models, then download the preferred.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available models:\n",
      "0: tiny.en\n",
      "1: tiny\n",
      "2: base.en\n",
      "3: base\n",
      "4: small.en\n",
      "5: small\n",
      "6: medium.en\n",
      "7: medium\n",
      "8: large-v1\n",
      "9: large-v2\n",
      "10: large\n"
     ]
    }
   ],
   "source": [
    "models = whisper.available_models()\n",
    "id = get_models(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = run_model(\"ctranslate\",id[9],4,\"cuda\")\n",
    "#model2 = whisper.load_model(\"tiny\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run and time execution of transcription\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcribing 1\n",
      "\n",
      "Detected language 'ja' with probability 0.953613\n",
      "[0.00s -> 0.82s] また明日!\n",
      "items in list: ['また明日!']\n",
      "\n",
      "\n",
      "Elapsed time: 0.41118407249450684\n",
      "Average time: 0.41118407249450684 s\n",
      "Total time: 0.41118407249450684 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nlist2 = []\\n# Translate audio\\nprint(\"Transcribing 2\")\\nfor i in range(n):\\n    t1 = time.time()\\n    #text = transcribe_fast(model,file,5,True,True)\\n    segments, info = model.transcribe(audio=file,beam_size=5,word_timestamps=True,task=\"translate\")\\n    print(\"\\nDetected language \\'%s\\' with probability %f\" % (info.language, info.language_probability))\\n\\n    # Print out segments\\n    for segment in segments:\\n        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\\n        list2.append(segment.text)\\n        print(f\"items in list: {list2}\")\\n    t2 = time.time()\\n    print(\"\\n\\nElapsed time:\", t2-t1)\\n    t_avg.append(t2-t1)\\nprint(f\"Average time: {np.sum(t_avg)/n} s\")\\nprint(f\"Total time: {np.sum(t_avg)} s\")\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = \"../Audio/jap.wav\"\n",
    "file = \"../Audio/jap4.wav\"\n",
    "t_avg = []\n",
    "list = []\n",
    "n = 1\n",
    "\n",
    "# Transcribe audio\n",
    "print(\"Transcribing 1\")\n",
    "for i in range(n):\n",
    "    t1 = time.time()\n",
    "    #text = transcribe_fast(model,file,5,True,True)\n",
    "    segments, info = model.transcribe(audio=file,beam_size=5,word_timestamps=True,task=\"transcribe\")\n",
    "    print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "    t2 = time.time()\n",
    "\n",
    "    # Print out segments\n",
    "    for segment in segments:\n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "        list.append(segment.text)\n",
    "        print(f\"items in list: {list}\")\n",
    "    print(\"\\n\\nElapsed time:\", t2-t1)\n",
    "    t_avg.append(t2-t1)\n",
    "print(f\"Average time: {np.sum(t_avg)/n} s\")\n",
    "print(f\"Total time: {np.sum(t_avg)} s\")\n",
    "\n",
    "\"\"\"\n",
    "list2 = []\n",
    "# Translate audio\n",
    "print(\"Transcribing 2\")\n",
    "for i in range(n):\n",
    "    t1 = time.time()\n",
    "    #text = transcribe_fast(model,file,5,True,True)\n",
    "    segments, info = model.transcribe(audio=file,beam_size=5,word_timestamps=True,task=\"translate\")\n",
    "    print(\"\\nDetected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "    # Print out segments\n",
    "    for segment in segments:\n",
    "        print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
    "        list2.append(segment.text)\n",
    "        print(f\"items in list: {list2}\")\n",
    "    t2 = time.time()\n",
    "    print(\"\\n\\nElapsed time:\", t2-t1)\n",
    "    t_avg.append(t2-t1)\n",
    "print(f\"Average time: {np.sum(t_avg)/n} s\")\n",
    "print(f\"Total time: {np.sum(t_avg)} s\")\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "#while 1:\n",
    "#    #audio_data = audio_buffer.get()\n",
    "#    t1 = time.time()\n",
    "#    text = transcribe_fast(model,file,5,True,True)\n",
    "#    t2 = time.time()\n",
    "#    print(\"\\nElapsed time:\", t2-t1, end=\"\")\n",
    "    \n",
    "#    if keyboard.is_pressed(\"w\"):\n",
    "#            print(\"Closing thread...\")\n",
    "#            print(\"\\nStopping...\")\n",
    "#            sys.exit()\n",
    "#            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "また明日!\n"
     ]
    }
   ],
   "source": [
    "for i in list:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'translate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mg:\\Github\\PRIVATE PROJECTS\\Real-Time-Speech-To-Text\\tests\\Model_Transcription_Test.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/g%3A/Github/PRIVATE%20PROJECTS/Real-Time-Speech-To-Text/tests/Model_Transcription_Test.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtranslate\u001b[39;00m \u001b[39mimport\u001b[39;00m Translator\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Github/PRIVATE%20PROJECTS/Real-Time-Speech-To-Text/tests/Model_Transcription_Test.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m translator\u001b[39m=\u001b[39m Translator(to_lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39men\u001b[39m\u001b[39m\"\u001b[39m,from_lang\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mja\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/g%3A/Github/PRIVATE%20PROJECTS/Real-Time-Speech-To-Text/tests/Model_Transcription_Test.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mlist\u001b[39m:\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'translate'"
     ]
    }
   ],
   "source": [
    "from translate import Translator\n",
    "translator= Translator(to_lang=\"en\",from_lang=\"ja\")\n",
    "for i in list:\n",
    "    t1 = time.time()\n",
    "    translation = translator.translate(i)\n",
    "    print(translation)\n",
    "    print(\"time\",time.time()-t1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from deep_translator import GoogleTranslator, MyMemoryTranslator, LibreTranslator,batch_detection\n",
    "t1 = time.time()\n",
    "trans = GoogleTranslator(\"ja\",\"en\").translate_batch(list)\n",
    "print(trans)\n",
    "print(\"time\",time.time()-t1)\n",
    "\n",
    "t1 = time.time()\n",
    "trans = GoogleTranslator(\"ja\",\"en\").translate(str(list))\n",
    "print(trans)\n",
    "print(\"time\",time.time()-t1)\n",
    "print(trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in list:\n",
    "    t1 = time.time()\n",
    "    translated = MyMemoryTranslator(source='ja', target='en').translate_batch(str(i))\n",
    "    print(translated)\n",
    "    print(\"time\",time.time()-t1)\n",
    "    \n",
    "    t1 = time.time()\n",
    "    translated = MyMemoryTranslator(source='ja', target='en').translate(str(list))\n",
    "    print(translated)\n",
    "    print(\"time\",time.time()-t1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slow and bad accuracy\n",
    "#translated = LibreTranslator(source='ja', target='en').translate_batch(list)\n",
    "#print(translated)\n",
    "#translated = LibreTranslator(source='ja', target='en').translate(str(list))\n",
    "#print(translated)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate memory usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = psutil.Process()\n",
    "memory_info = pid.memory_info()\n",
    "print(\"Memory usage:\", memory_info.rss / 1024 / 1024, \"MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
